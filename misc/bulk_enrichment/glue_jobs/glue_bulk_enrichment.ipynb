{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.2X\n%number_of_workers 10\n%additional_python_modules  langdetect, datasketch, kshingle, beautifulsoup4,htmldate,wordninja, torch,keybert, transformers==4.20.1, python_docx, docx,spacy, pikepdf, PyPDF2, openpyxl, PyMuPDF, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/pdfminer.six-20221105-py3-none-any.whl, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/word_forms-2.1.0-py3-none-any.whl, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/en_core_web_sm-3.5.0-py3-none-any.whl, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/en_core_web_lg-3.5.0-py3-none-any.whl\n%extra_py_files s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/date_generation.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/text_hashing.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/document_type_identification.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/docx_to_text.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/html_to_text.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/keyword_extraction.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/legislative_origin.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/odf_to_text.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/pdf_to_text.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/title_generation.zip, s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/python_modules/summarisation.zip\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "DATA_SOURCE_PREFIX='temp'\nDATA_SOURCE_BUCKET_NAME='beis-dev-datalake'\nDTI_RULEBOOK='dti/doc_type_rules_v.2.jsonl'\nTOPIC_MAPPING_FILE_PATH='s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/resources/topic_id_mapping.parquet'\nRESOURCES_BUCKET='s3://aws-glue-assets-412071276468-eu-west-2/glue_resources/resources/'\nPROCESSED_METADATA_BUCKET=f's3://{DATA_SOURCE_BUCKET_NAME}/glue_processed_metadata/'",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "import nltk\nsc.addFile(RESOURCES_BUCKET,True)\n\nimport pandas as pd\nimport spacy\nimport json\nfrom pdf_to_text.pdf_to_text import pdf_converter\nfrom odf_to_text.odf_to_text import odf_converter\nfrom docx_to_text.docx_to_text import docx_converter\nfrom html_to_text.html_to_text import html_converter\nfrom text_hashing.hashing import create_hash\nfrom date_generation.date_generation import date_generation\nfrom legislative_origin.lo_extraction import lo_extraction\nfrom title_generation.title_generation import title_generator\nfrom document_type_identification.rule_based_dti import dti\nfrom keyword_extraction.keyword_extraction import  keyword_extraction\nfrom summarisation.summarisation import summarizer\nimport boto3\nimport io\nfrom pyspark.sql.types import StructType,StructField, StringType,ArrayType\nfrom pyspark import SparkFiles\nfrom datetime import datetime\nimport pyspark.sql.functions as F\nfrom uuid import uuid4\n\ns3_rsc=boto3.resource('s3')\ns3_cli=boto3.client('s3')\n\ndoc_format_map = {\n    'PDF': pdf_converter,\n    'ODF': odf_converter,\n    'ODT': odf_converter,\n    'DOCX': docx_converter,\n    # 'doc': docx_converter,\n    'HTML': html_converter\n}\nmd_schema = StructType([\n        StructField(\"raw_uri\", StringType(), True),\n        StructField(\"text\", StringType(), True),\n        StructField(\"uri\", StringType(), True),\n        StructField(\"title\", StringType(), True),\n        StructField(\"date_published\", StringType(), True),\n        StructField(\"document_uid\", StringType(), True),\n        StructField(\"regulator_id\", StringType(), True),\n        StructField(\"summary\", StringType(), True),\n        StructField(\"language\", StringType(), True),\n        StructField(\"document_type\", StringType(), True),\n        StructField(\"hash_text\", StringType(), True),\n        StructField(\"regulatory_topic\",ArrayType(StringType()), True),\n        StructField(\"assigned_orp_topic\",ArrayType(StringType()), True),\n        StructField(\"legislative_origins\", ArrayType(\n            StructType([\n                StructField(\"title\", StringType(), True),\n                StructField(\"ref\", StringType(), True),\n                StructField(\"href\", StringType(), True),\n                StructField(\"number\", StringType(), True),\n                StructField(\"division\", StringType(), True),\n                StructField(\"type\", StringType(), True)\n             ]))),\n        StructField(\"keywords\",ArrayType(StringType()), True),\n    \n        ])\n\nnull_ret = ([None]*10)+[[],[], [], []]",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "rule_json = s3_cli.get_object(Bucket=DATA_SOURCE_BUCKET_NAME, Key=DTI_RULEBOOK)['Body'].read().decode('utf-8')\ndti_patterns =[json.loads(line) for line in rule_json.split('\\n') if line.strip()]\nnlp = spacy.load(\"en_core_web_sm\", exclude=['entity_ruler',  'ner'])\nnlp.add_pipe(\"entity_ruler\", config={'phrase_matcher_attr':'LOWER'}).add_patterns(dti_patterns)\n\ndef download_text(s3_client, object_key, source_bucket):\n        '''Downloads the PDF from S3 ready for conversion and metadata extraction'''\n\n        document = s3_client.get_object(\n            Bucket=source_bucket,\n            Key=object_key\n        )['Body'].read()\n\n        doc_bytes_io = io.BytesIO(document)\n        return doc_bytes_io\n\ndef get_reg_id(uri, doc_format):\n    if doc_format == 'HTML':\n        return 'hse' if 'hse.gov.uk' in uri else 'ea'\n    else: return uri.split('/')[1].lower()\n\n# convert topics into graph-friendly topics \ntopic_df = pd.read_parquet(TOPIC_MAPPING_FILE_PATH)\ndef get_topic_path(topics):\n    ftopics = []\n    assigned = []\n    for topic in topics:\n        idx = topic_df.where(topic_df==topic).dropna(how='all').dropna(axis=1).index\n        if list(idx):\n            assigned.append(topic_df.loc[idx[0]].path_id)\n            t= assigned[-1].split('/')\n            ftopics.extend(['/'.join(t[:i]) for i in range(2,len(t)+1)])\n    return list(set(ftopics)), assigned\n\ndef extract_data( uri, parent_uri, doc_format, topics, nlp):\n    try:\n        s3 = boto3.client('s3')\n        nltk.data.path.append(SparkFiles.get('resources/nltk_data'))\n        btext = uri if doc_format=='HTML' else download_text(s3, uri, DATA_SOURCE_BUCKET_NAME)\n        text, title, date_published = doc_format_map[doc_format](btext)\n        document_uid = uuid4().hex\n        reg_id =  get_reg_id(uri, doc_format)\n        summary, lang = summarizer(text)\n        ntitle = title_generator(text, title)\n        ndp = date_generation(text, date_published)\n        los = lo_extraction(text)\n        keywords = keyword_extraction(text, title)\n        document_type = dti(uri, parent_uri, text, ntitle, nlp)\n        hash_text = create_hash(text)\n        nuri = uri if doc_format=='HTML' else f'bulk/{uri.split(\"/\")[-1]}'\n        reg_topic, assigned = get_topic_path(topics)\n        return uri,text, nuri, ntitle, ndp, document_uid, reg_id, summary,lang, document_type, hash_text,reg_topic, assigned, los, keywords\n    except Exception as e:\n        print(f'ERROR: {uri} \\t{doc_format}')\n        print(f'ERR.BODY:\\n{e}')\n        return [uri] + null_ret\n\n    ",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "# Import data from S3 into pyspark dataframe\nflist=[obj.key for obj in s3_rsc.Bucket(DATA_SOURCE_BUCKET_NAME).objects.all() if obj.key.startswith(DATA_SOURCE_PREFIX)]\n\ndf= pd.DataFrame(flist, columns=['raw_uri'])\next_type=('pdf','docx','odt','odf', 'html')\ndf['document_format'] =  df.raw_uri.apply(lambda x: 'dir' if x.endswith('/') else x.split('.')[-1])\n\n# get parent_uri\ncolname = {\n2:['topics','uri'],\n3:['topics','parent_uri', 'uri'] ,\n4:['topics','parent_uri','org_uri', 'uri'] }\np_ext = df[df.document_format=='parquet'].raw_uri\ndf_ext = pd.DataFrame()\nfor lk in p_ext:\n    a = pd.read_parquet(download_text(s3_cli, lk, DATA_SOURCE_BUCKET_NAME))\n    a.columns =  colname[a.shape[1]]\n    df_ext=pd.concat([df_ext, a])\ndf['uri'] = df.apply(lambda x: x.raw_uri if x.document_format=='html' else x.raw_uri.split('/')[-1].split('.')[0], axis=1)\n\ndf = df.merge(df_ext, on='uri', how='left')\ndf_ext = df_ext[df_ext.uri.apply(str.startswith, args=['http'])]\ndf_ext['document_format']='html'\ndf_ext.rename(columns={'uri':'raw_uri'}, inplace=True)\ndf = pd.concat([df,df_ext])\ndf=df.drop('uri', axis=1)\n\ndf = df[df.document_format.isin(ext_type)].reset_index(drop=True)\ndf.document_format = df.document_format.apply(str.upper)\n\n\ndf.topics=df.topics.apply(lambda x: [x] if type(x)==str else list(x))",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "schema = StructType([\n        StructField(\"raw_uri\", StringType(), True),\n        StructField(\"document_format\", StringType(), True),\n        StructField(\"topics\",ArrayType(StringType()), True),\n        StructField(\"parent_uri\", StringType(), True)\n])\n\nDF = spark.createDataFrame(df, schema=schema)\nstatic_md = [\n('date_uploaded', datetime.now().isoformat()),\n    ('status','published'),\n    ('user_id','bulk_uploader'),\n    ('version', 1)\n]\nfor k,v in static_md:\n    DF = DF.withColumn(k, F.lit(v))",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "out = DF.rdd.map(lambda x: extract_data(x['raw_uri'],x['parent_uri'], x['document_format'],x['topics'], nlp))\n\ndf2=out.toDF(schema=md_schema)\n\ndff = DF.join(df2, on='raw_uri', how='outer')\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "from time import time\nt = time()\ndff.write.mode('overwrite').parquet(PROCESSED_METADATA_BUCKET)\nprint(f'JOB DONE: {time()-t} sec')",
			"metadata": {
				"trusted": true
			},
			"execution_count": null,
			"outputs": []
		}
	]
}