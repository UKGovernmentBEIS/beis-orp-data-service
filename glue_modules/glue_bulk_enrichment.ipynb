{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%stop_session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 2\n%additional_python_modules  datasketch, kshingle, beautifulsoup4,htmldate,wordninja, torch,keybert, transformers, python_docx, docx,spacy, pikepdf, PyPDF2, openpyxl, PyMuPDF, s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/pdfminer.six-20221105-py3-none-any.whl, s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/word_forms-2.1.0-py3-none-any.whl, s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/en_core_web_sm-3.5.0-py3-none-any.whl, s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/en_core_web_lg-3.5.0-py3-none-any.whl\n%extra_py_files s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/date_generation.zip, s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/text_hashing.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/document_type_identification.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/docx_to_text.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/html_to_text.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/keyword_extraction.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/legislative_origin.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/odf_to_text.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/pdf_to_text.zip,    s3://aws-glue-assets-455762151948-eu-west-2/notebooks/job_wheels/title_generation.zip\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\nlogger = glueContext.get_logger()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SOURCE_PREFIX='temp'\nDATA_SOURCE_BUCKET_NAME='beis-orp-dev-datalake'\nDTI_RULEBOOK='dti/doc_type_rules_v.2.jsonl'\nRESOURCES_BUCKET='s3://aws-glue-assets-455762151948-eu-west-2/notebooks/resources/'\nPROCESSED_METADATA_BUCKET=f's3://{DATA_SOURCE_BUCKET_NAME}/glue_processed_metadata/'\n\nimport nltk\nsc.addFile(RESOURCES_BUCKET,True)\n\nimport pandas as pd\nimport spacy\nimport json\nfrom pdf_to_text.pdf_to_text import pdf_converter\nfrom odf_to_text.odf_to_text import odf_converter\nfrom docx_to_text.docx_to_text import docx_converter\nfrom html_to_text.html_to_text import html_converter\nfrom text_hashing.hashing import create_hash\nfrom date_generation.date_generation import date_generation\nfrom legislative_origin.lo_extraction import lo_extraction\nfrom title_generation.title_generation import title_generator\nfrom document_type_identification.rule_based_dti import dti\nfrom keyword_extraction.keyword_extraction import  keyword_extraction\nimport boto3\nimport io\nfrom pyspark.sql.types import StructType,StructField, StringType,ArrayType\nfrom pyspark import SparkFiles\nfrom datetime import datetime\nimport pyspark.sql.functions as F\nfrom uuid import uuid4\n\ns3_rsc=boto3.resource('s3')\ns3_cli=boto3.client('s3')\n\ndoc_format_map = {\n    'pdf': pdf_converter,\n    'odf': odf_converter,\n    'docx': docx_converter,\n    # 'doc': docx_converter,\n    'html': html_converter\n}\nmd_schema = StructType([\n        StructField(\"raw_uri\", StringType(), True),\n        StructField(\"uri\", StringType(), True),\n        StructField(\"title\", StringType(), True),\n        StructField(\"date_published\", StringType(), True),\n        StructField(\"document_uid\", StringType(), True),\n        StructField(\"regulator_id\", StringType(), True),\n        StructField(\"summary\", StringType(), True),\n        StructField(\"document_type\", StringType(), True),\n        StructField(\"hash_text\", StringType(), True),\n        StructField(\"legislative_origins\", ArrayType(\n            StructType([\n                StructField(\"title\", StringType(), True),\n                StructField(\"ref\", StringType(), True),\n                StructField(\"href\", StringType(), True),\n                StructField(\"number\", StringType(), True),\n                StructField(\"division\", StringType(), True),\n                StructField(\"type\", StringType(), True)\n             ]))),\n        StructField(\"keywords\",ArrayType(StringType()), True),\n    \n        ])\n\nnull_ret = ([None]*9)+[[],[]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rule_json = s3_cli.get_object(Bucket=DATA_SOURCE_BUCKET_NAME, Key=DTI_RULEBOOK)['Body'].read().decode('utf-8')\ndti_patterns =[json.loads(line) for line in rule_json.split('\\n') if line.strip()]\nnlp = spacy.load(\"en_core_web_sm\", exclude=['entity_ruler',  'ner'])\nnlp.add_pipe(\"entity_ruler\", config={'phrase_matcher_attr':'LOWER'}).add_patterns(dti_patterns)\n\ndef download_text(s3_client, object_key, source_bucket):\n        '''Downloads the PDF from S3 ready for conversion and metadata extraction'''\n\n        document = s3_client.get_object(\n            Bucket=source_bucket,\n            Key=object_key\n        )['Body'].read()\n\n        doc_bytes_io = io.BytesIO(document)\n        return doc_bytes_io\n\ndef get_reg_id(uri, doc_format):\n    if doc_format == 'html':\n        return 'hse' if 'hse.gov.uk' in uri else 'ea'\n    else: return uri.split('/')[1].lower()\n\ndef extract_data( uri, doc_format, nlp):\n    try:\n        s3 = boto3.client('s3')\n        nltk.data.path.append(SparkFiles.get('resources/nltk_data'))\n        btext = uri if doc_format=='html' else download_text(s3, uri, DATA_SOURCE_BUCKET_NAME)\n        text, title, date_published = doc_format_map[doc_format](btext)\n        document_uid = uuid4().hex\n        reg_id =  get_reg_id(uri, doc_format)\n        summary = None\n        ntitle = title_generator(text, title)\n        ndp = date_generation(text, date_published)\n        los = lo_extraction(text)\n        keywords = keyword_extraction(text, title)\n        document_type = dti(text, ntitle, nlp)\n        hash_text = create_hash(text)\n        nuri = uri if doc_format=='html' else f'bulk/{uri.split(\"/\")[-1]}'\n        return uri, nuri, ntitle, ndp, document_uid, reg_id, summary, document_type, hash_text, los, keywords\n    except Exception as e:\n        print(f'ERROR: {uri} \\t{doc_format}')\n        print(f'ERR.BODY:\\n{e}')\n        return null_ret\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import data from S3 into pyspark dataframe\nflist=[obj.key for obj in s3_rsc.Bucket(DATA_SOURCE_BUCKET_NAME).objects.all() if obj.key.startswith(DATA_SOURCE_PREFIX)]\n\ndf= pd.DataFrame(flist, columns=['raw_uri'])\next_type=('pdf','docx','odt','odf', 'html')\ndf['document_format'] =  df.raw_uri.apply(lambda x: 'dir' if x.endswith('/') else x.split('.')[-1])\n\nlinks = df[df.document_format=='xlsx'].raw_uri\ndff = pd.DataFrame()\nfor lk in links:\n    dff=pd.concat([dff, pd.read_excel(download_text(s3_cli, lk, DATA_SOURCE_BUCKET_NAME))])\ndff.columns=['regulatory_topic','raw_uri']\ndff['document_format']='html'\ndf = pd.concat([df,dff.drop('regulatory_topic', axis=1)])\ndf = df[df.document_format.isin(ext_type)].reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DF = spark.createDataFrame(df.head())\n\nstatic_md = [\n('date_uploaded', datetime.now().isoformat()),\n    ('status','published'),\n    ('user_id','bulk_uploader'),\n    ('version', 1)\n]\nfor k,v in static_md:\n    DF = DF.withColumn(k, F.lit(v))\n    \nDF.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = DF.rdd.map(lambda x: extract_data(x['raw_uri'], x['document_format'], nlp))\n\ndf2=out.toDF(schema=md_schema)\n\ndff = DF.join(df2, on='raw_uri', how='inner')\n# dff.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dff.write.mode('overwrite').parquet(PROCESSED_METADATA_BUCKET)","metadata":{},"execution_count":null,"outputs":[]}]}