{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to download pdfs from links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rj/t9tbtlf12d9_bn6nhb1f0tkc0000gq/T/ipykernel_81480/2843047389.py:25: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xlsxwriter\n",
    "import pdfminer\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "from datetime import date\n",
    "TODAY_STR = date.today().strftime(\"%d%m%y\")\n",
    "\n",
    "# Set max column widths\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "DATE_STR = \"121022\"\n",
    "industryPDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{DATE_STR}-industry_PDF.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")\n",
    "topicsPDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{DATE_STR}-topics_PDFs.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")\n",
    "COSHH_PDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{DATE_STR}-COSHH_PDFs.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")\n",
    "publications_PDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{DATE_STR}-Publications_PDFs.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")\n",
    "catelogue_PDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{DATE_STR}-catelogue_PDFs.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")\n",
    "construction_PDFs_df = pd.read_excel(f\"/Users/thomas/Documents/BEIS/scraper/scraped_data/{TODAY_STR}-construction_PDFs.xlsx\", engine = \"openpyxl\").drop(columns = \"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pdfs is 927\n"
     ]
    }
   ],
   "source": [
    "# Column to say which dataframe it's from\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "    \n",
    "list_of_dfs = [industryPDFs_df, topicsPDFs_df, COSHH_PDFs_df, publications_PDFs_df, catelogue_PDFs_df, construction_PDFs_df]\n",
    "for df in list_of_dfs:\n",
    "    df[\"Source\"] = namestr(df, globals())[0]\n",
    "\n",
    "# Combine dfs and drop duplicates based on pdf name\n",
    "combined_PDF_df = pd.concat([industryPDFs_df, topicsPDFs_df, COSHH_PDFs_df, publications_PDFs_df, catelogue_PDFs_df, construction_PDFs_df]).drop_duplicates(subset = \"PDFs\", keep = \"last\").reset_index(drop = True)\n",
    "\n",
    "print(f\"Number of unique pdfs is {len(combined_PDF_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many PDF links haven't been constructed properly due to relative links so are constructed now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace where there are double forward slashes in URLs\n",
    "combined_PDF_df[\"URLs\"] = combined_PDF_df[\"URLs\"].apply(lambda x : x.replace(\"//\", \"/\"))\n",
    "combined_PDF_df[\"URLs\"] = combined_PDF_df[\"URLs\"].apply(lambda x : x.replace(\"https:/www.hse.gov.uk/\", \"https://www.hse.gov.uk/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to call pdf, if you can't then add string after https://www.hse.gov.uk/ from URL to the PDFs\n",
    "for loc in combined_PDF_df.index:\n",
    "    if requests.get(combined_PDF_df[\"PDFs\"].iloc[loc]).status_code == 404:\n",
    "        # If https appears twice, get rid of 'https://www.hse.gov.uk/'\n",
    "        counter = 0\n",
    "        for string in combined_PDF_df[\"PDFs\"].iloc[loc].split(\"/\"):\n",
    "            if \"https:\" == string or \"http:\" == string:\n",
    "                counter += 1\n",
    "        if counter == 2:\n",
    "            combined_PDF_df[\"PDFs\"].iloc[loc] = combined_PDF_df[\"PDFs\"].iloc[loc].replace(\"https://www.hse.gov.uk/\", \"\", 1)\n",
    "        # If there is no / after pubns, add it\n",
    "        elif \"pubns\" in combined_PDF_df[\"PDFs\"].iloc[loc] and \"pubns/\" not in combined_PDF_df[\"PDFs\"].iloc[loc]:\n",
    "            combined_PDF_df[\"PDFs\"].iloc[loc] = combined_PDF_df[\"PDFs\"].iloc[loc].replace(\"pubns\", \"pubns/\")\n",
    "        else:\n",
    "            combined_PDF_df[\"PDFs\"].iloc[loc] = 'https://www.hse.gov.uk/' + combined_PDF_df[\"URLs\"].iloc[loc].split(\"/\")[3] + \"/\" + combined_PDF_df[\"PDFs\"].iloc[loc].replace('https://www.hse.gov.uk/', \"\")\n",
    "    # Replace double pubns if they appear\n",
    "        second_counter = 0\n",
    "        for string in combined_PDF_df[\"PDFs\"].iloc[loc].split(\"/\"):\n",
    "            if \"pubns\" == string:\n",
    "                second_counter += 1\n",
    "        if counter == 2:\n",
    "            combined_PDF_df[\"PDFs\"].iloc[loc] = combined_PDF_df[\"PDFs\"].iloc[loc].replace(\"pubns/pubns/\", \"pubns/\")\n",
    "        if combined_PDF_df[\"PDFs\"].iloc[loc][0] != \"h\":\n",
    "            combined_PDF_df[\"PDFs\"].iloc[loc]  = \"/\".join(combined_PDF_df[\"PDFs\"].iloc[loc].split(\"/\")[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all pdfs which can't be accessed still\n",
    "list_of_failed_links = [pdf for pdf in combined_PDF_df[\"PDFs\"] if requests.get(pdf).status_code == 404]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For now I am ignoring failed links pdfs in the interests of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut failed links from the dataframe\n",
    "pdfs_to_download = combined_PDF_df[~combined_PDF_df[\"PDFs\"].isin(list_of_failed_links)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pdfs to /Users/thomas/Documents/BEIS/input_data/pdfs\n",
    "# importing PdfFileWriter class\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "for pdf in pdfs_to_download[\"PDFs\"]:\n",
    "    response = requests.get(pdf)\n",
    "    with open(os.path.join(\"/Users/thomas/Documents/BEIS/input_data/pdfs/\" + pdf.replace(\"/\", \"rand_str\")), \"wb\") as file:\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read pdfs from the directory and output to a dataframe\n",
    "def read_pdf_from_directory(dir)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from pdfs and adds a column in the df with the textual data\n",
    "    \"\"\"\n",
    "    pdf_columns = []\n",
    "    for pdf in os.listdir(dir):\n",
    "        file = os.path.join(dir, pdf)\n",
    "        try:\n",
    "            i_f = open(file,'rb')\n",
    "            resMgr = PDFResourceManager()\n",
    "            retData = io.StringIO()\n",
    "            txtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
    "            interpreter = PDFPageInterpreter(resMgr, txtConverter)\n",
    "            for page in PDFPage.get_pages(i_f):\n",
    "                interpreter.process_page(page)\n",
    "            txt = retData.getvalue() \n",
    "            string_txt = str(txt)\n",
    "            pdf_columns.append({\"PDFs\" : pdf, \"Secondary Legislation\" : string_txt}) \n",
    "        except:\n",
    "            continue\n",
    "    pdf_df = pd.DataFrame(pdf_columns)\n",
    "    pdf_df[\"PDFs\"] = pdf_df[\"PDFs\"].apply(lambda x: x.replace(\"rand_str\", \"/\"))\n",
    "    return pdf_df\n",
    "\n",
    "pdfminer_df = read_pdf_from_directory(\"/Users/thomas/Documents/BEIS/input_data/pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get additional pdfs\n",
    "additional_pdfminer_df = read_pdf_from_directory(\"/Users/thomas/Documents/BEIS/input_data/additional_pdfs/\")\n",
    "additional_pdfminer_df[\"Source\"] = \"Health and Safety Law\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy PDF df and add column for urls\n",
    "final_df = pd.concat([pdfminer_df, additional_pdfminer_df])\n",
    "final_df = final_df.merge(combined_PDF_df, how = \"left\", on = \"PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to dataframe\n",
    "final_df.to_excel(f\"/Users/thomas/Documents/BEIS/input_data/{TODAY_STR}-pdf_rawdata.xlsx\", engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('scraperenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "159f4b9e810364f9108e48717d9724edba5849d39959c67371176a11ea52020f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
