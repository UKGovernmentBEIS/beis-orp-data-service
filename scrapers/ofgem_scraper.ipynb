{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape and download PDFs from Ofgem website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from urllib import request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "from datetime import date\n",
    "from typing import List\n",
    "from xmlrpc.client import Boolean\n",
    "from selenium import webdriver\n",
    "from pathlib import Path\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import staleness_of, StaleElementReferenceException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pdfminer\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "import io\n",
    "\n",
    "TODAY_STR = date.today().strftime(\"%d%m%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url to be scraped\n",
    "URL = \"https://www.ofgem.gov.uk/master-publications-library?sort=publication_date\"\n",
    "URL_list = []\n",
    "\n",
    "# All links to be scraped\n",
    "for page in range(1, 428):\n",
    "    new_url = \"https://www.ofgem.gov.uk/master-publications-library?sort=publication_date\"+\"&page=\"+str(page)\n",
    "    URL_list.append(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define driver\n",
    "driver = webdriver.Chrome(\"/Users/thomas/opt/anaconda3/envs/scraperenv/lib/python3.10/site-packages/chromedriver_autoinstaller/106/chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape each link on given page\n",
    "\n",
    "web_links = []\n",
    "\n",
    "for url in URL_list:\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    links = []\n",
    "\n",
    "    for num_links in range(1, 51):\n",
    "        \n",
    "        link = driver.find_elements_by_xpath(f'/html/body/div[3]/div/div/main/div[2]/div/div[2]/div/div/div[2]/div/div/div/div[2]/div/ol/li[{num_links}]/div/a')\n",
    "        # link = WebDriverWait(driver, 20).until(lambda x: x.find_elements_by_xpath(f'/html/body/div[3]/div/div/main/div[2]/div/div[2]/div/div/div[2]/div/div/div/div[2]/div/ol/li[{num_links}]/div/a'))\n",
    "        links.append(link)\n",
    "\n",
    "    for href in links:\n",
    "        for l in href:\n",
    "            web_links.append(l.get_attribute('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how many guidance documents there are\n",
    "counter = 0\n",
    "for ul in web_links:\n",
    "    if \"guidance\" in ul:\n",
    "        counter+=1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access link if guidance contained in link and scrape pdf link from the link and download the pdf\n",
    "counter = 0\n",
    "for url in web_links:\n",
    "    if \"guidance\" in url:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        links = []\n",
    "        \n",
    "        link = driver.find_elements_by_xpath(f'/html/body/div[3]/div/div/main/div[2]/div/div[2]/div/div[1]/div/div[2]/div[2]/div/a')\n",
    "        links.append(link)\n",
    "\n",
    "        web_links = []\n",
    "        for href in links:\n",
    "            for l in href:\n",
    "                web_links.append(l.get_attribute('href'))\n",
    "        pdf_links = list(set([pdf for pdf in web_links if \".pdf\" in pdf]))\n",
    "\n",
    "        # Download the pdf\n",
    "        for pdf in pdf_links:\n",
    "            counter += 1\n",
    "            response = requests.get(pdf)\n",
    "            with open(os.path.join(\"/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/\" + str(counter) + \".pdf\"), \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/412.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/416.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/414.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/428.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/459.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/462.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/480.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/426.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n",
      "The PDF <_io.BufferedReader name='/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs2/423.pdf'> contains a metadata field indicating that it should not allow text extraction. Ignoring this field and proceeding. Use the check_extractable if you want to raise an error in this case\n"
     ]
    }
   ],
   "source": [
    "# Read pdfs from the directory and output to a dataframe\n",
    "def read_pdf_from_directory(dir)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads data from pdfs and adds a column in the df with the textual data\n",
    "    \"\"\"\n",
    "    pdf_columns = []\n",
    "    for pdf in os.listdir(dir):\n",
    "        file = os.path.join(dir, pdf)\n",
    "        try:\n",
    "            i_f = open(file,'rb')\n",
    "            resMgr = PDFResourceManager()\n",
    "            retData = io.StringIO()\n",
    "            txtConverter = TextConverter(resMgr,retData, laparams= LAParams())\n",
    "            interpreter = PDFPageInterpreter(resMgr, txtConverter)\n",
    "            for page in PDFPage.get_pages(i_f):\n",
    "                interpreter.process_page(page)\n",
    "            txt = retData.getvalue() \n",
    "            string_txt = str(txt)\n",
    "            pdf_columns.append({\"PDFs\" : pdf, \"Secondary Legislation\" : string_txt}) \n",
    "        except:\n",
    "            continue\n",
    "    pdf_df = pd.DataFrame(pdf_columns)\n",
    "    return pdf_df\n",
    "\n",
    "pdfminer_df = read_pdf_from_directory(\"/Users/thomas/Documents/BEIS/input_data/ofgem_pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to dataframe\n",
    "pdfminer_df.to_excel(f\"/Users/thomas/Documents/BEIS/input_data/{TODAY_STR}-pdf_ofgem_rawdata.xlsx\", engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('scraperenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "159f4b9e810364f9108e48717d9724edba5849d39959c67371176a11ea52020f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
